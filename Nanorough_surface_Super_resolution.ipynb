{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nanorough surface Super-resolution.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMLKGioNpE1kHymYpMVtizo",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/billsioros/thesis/blob/master/Nanorough_surface_Super_resolution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XIigYPTAMAH"
      },
      "source": [
        "# Prerequisites"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1mdRrYGM60R"
      },
      "source": [
        "## Pip Modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fDiW2N-M60S",
        "outputId": "94082f1c-aa8d-441c-fff6-d4a15287ade1"
      },
      "source": [
        "!pip install torch numpy sympy scipy plotly pandas install matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.7/dist-packages (1.7.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (1.4.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n",
            "Collecting install\n",
            "  Downloading https://files.pythonhosted.org/packages/f0/a5/fd2eb807a9a593869ee8b7a6bcb4ad84a6eb31cef5c24d1bfbf7c938c13f/install-1.3.4-py3-none-any.whl\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.7/dist-packages (from sympy) (1.2.1)\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from plotly) (1.3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from plotly) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.1)\n",
            "Installing collected packages: install\n",
            "Successfully installed install-1.3.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WktNsUHMAS63"
      },
      "source": [
        "## RNG Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1mLAArpLX7A"
      },
      "source": [
        "We are going to be seed the random number generator engines, so that our results are reproducible accross different set ups"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMnousxlAbPV"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "\n",
        "SEED = 1234\n",
        "\n",
        "if SEED is not None:\n",
        "  np.random.seed(SEED)\n",
        "  random.seed(SEED)\n",
        "  torch.manual_seed(SEED)\n",
        "  torch.cuda.manual_seed(SEED)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  os.environ['PYTHONHASHSEED'] = str(SEED)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxhexKDEM60T"
      },
      "source": [
        "## Determining the Current Working Directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oRdJanBM60T"
      },
      "source": [
        "from pathlib import Path\n",
        "\n",
        "BASE_DIR = Path.cwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ptZs2vpwNwJg"
      },
      "source": [
        "## Mounting Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXUKAqjuOvpK"
      },
      "source": [
        "GDRIVE_DIR = BASE_DIR / 'drive'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "musKsMGXNyQM",
        "outputId": "c288958c-ae8e-43e2-a03f-8acd32db5f99"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(f'{GDRIVE_DIR}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JflfjrdmDqL0"
      },
      "source": [
        "## Determining available backend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykoV8K_vDurq"
      },
      "source": [
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "  device = \"cuda:0\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYf2R4B1VgvI"
      },
      "source": [
        "device = torch.device(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUjCHTPY75SA"
      },
      "source": [
        "# General Utilities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcPlNw4r8AMB"
      },
      "source": [
        "from time import time\n",
        "from functools import wraps"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNezXDHEFGHj"
      },
      "source": [
        "## A debugging decorator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YGd3cTvW7_D2"
      },
      "source": [
        "import inspect\n",
        "\n",
        "def debug(method):\n",
        "    signature = inspect.signature(method)\n",
        "\n",
        "    defaults = {\n",
        "      k: v.default\n",
        "      for k, v in signature.parameters.items()\n",
        "      if v.default is not inspect.Parameter.empty\n",
        "    }\n",
        "\n",
        "    @wraps(method)\n",
        "    def wrapper(*args, **kwargs):\n",
        "        called_with = ''\n",
        "        if args:\n",
        "            called_with += ', '.join(str(x) for x in args)\n",
        "            called_with += ', '\n",
        "\n",
        "        called_with += ', '.join(f\"{x}={kwargs.get(x, defaults[x])}\" for x in defaults.keys())\n",
        "\n",
        "        try:\n",
        "          rv = method(*args, **kwargs)\n",
        "        except Exception as e:\n",
        "          print(f\"{method.__name__}({called_with}) raised {e}\")\n",
        "          raise\n",
        "\n",
        "        print(f\"{method.__name__}({called_with}) returned {rv}\")\n",
        "\n",
        "        return rv\n",
        "\n",
        "    return wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NGJXxECFLCK"
      },
      "source": [
        "## A benchmarking decorator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwcDSjQ6eeIV"
      },
      "source": [
        "def benchmark(method):\n",
        "  @wraps(method)\n",
        "  def wrapper(*args, **kwargs):\n",
        "    beg = time()\n",
        "    rv = method(*args, **kwargs)\n",
        "    end = time()\n",
        "\n",
        "    print(\"%s returned after %7.3f seconds\" % (method.__name__, (end - beg)))\n",
        "\n",
        "    return rv\n",
        "\n",
        "  return wrapper"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb9wKLZPvzA_"
      },
      "source": [
        "# Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKkmYI7AFSAY"
      },
      "source": [
        "## Correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzvfXfOSv0Cq"
      },
      "source": [
        "def correlation(z_ngs):\n",
        "  N = z_ngs.shape[0]\n",
        "  \n",
        "  rdif, hhcf1d = np.arange(N // 2), np.zeros(N // 2)\n",
        "\n",
        "  for ndif in range(N // 2):\n",
        "    surf1 = z_ngs[:N, : (N - ndif)]\n",
        "    surf2 = z_ngs[:N, ndif:N]\n",
        "    difsur2 = (surf1 - surf2) ** 2\n",
        "    hhcf1d[ndif] = np.sqrt(np.mean(np.mean(difsur2)))\n",
        "  \n",
        "  return rdif, hhcf1d"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YW7OFq-cu8JP"
      },
      "source": [
        "# Plotting Utilities\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hz7bcUAFVM9"
      },
      "source": [
        "## Plotting the correlation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWflQwpwz1ZV"
      },
      "source": [
        "import plotly.express as px\n",
        "\n",
        "def plot_correlation(array):\n",
        "  x, y = correlation(array)\n",
        "  \n",
        "  fig = px.line(\n",
        "    # title=\"1-D height-height correlation function\",\n",
        "    # x=\"r(nm)\", y=\"G(r) (nm)\",\n",
        "    x=x, y=y,\n",
        "    log_x=True, log_y=True\n",
        "  )\n",
        "  \n",
        "  fig.update_layout(\n",
        "    # title=title,\n",
        "    autosize=True,\n",
        "    width=500, height=500,\n",
        "    # margin=dict(l=65, r=50, b=65, t=90)\n",
        "  )\n",
        "\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhlgKN4rFZFZ"
      },
      "source": [
        "## Plotting a surface as a 3D surface"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajkmvyw7yiey"
      },
      "source": [
        "import plotly.graph_objects as go\n",
        "\n",
        "def as_3d_surface(array, autosize=False):\n",
        "  fig = go.Figure(data=[go.Surface(z=array)])\n",
        "\n",
        "  fig.update_layout(\n",
        "    # title=title,\n",
        "    autosize=True,\n",
        "    width=500, height=500,\n",
        "    # margin=dict(l=65, r=50, b=65, t=90)\n",
        "  )\n",
        "\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ps4YY-fGFdhi"
      },
      "source": [
        "## Plotting a surface as a grayscale image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oJjzoSyu-tw"
      },
      "source": [
        "import plotly.express as px\n",
        "\n",
        "def as_grayscale_image(array):\n",
        "  fig = px.imshow(array, color_continuous_scale='gray')\n",
        "  fig.update_layout(coloraxis_showscale=False)\n",
        "  fig.update_xaxes(showticklabels=False)\n",
        "  fig.update_yaxes(showticklabels=False)\n",
        "\n",
        "  fig.update_layout(\n",
        "    # title=title,\n",
        "    autosize=True,\n",
        "    width=500, height=500,\n",
        "    # margin=dict(l=65, r=50, b=65, t=90)\n",
        "  )\n",
        "\n",
        "  fig.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FHNoRSaFito"
      },
      "source": [
        "## Plotting a model's Learning Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_zheicWcFNh"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def train_vs_test(train_losses, test_losses):\n",
        "  x = list(range(max(len(train_losses), len(test_losses))))\n",
        "\n",
        "  plt.plot(x, train_losses, label='train')\n",
        "  plt.plot(x, test_losses, label='test')\n",
        "\n",
        "  plt.grid()\n",
        "  plt.title('Average MSE Loss / Epoch')\n",
        "  plt.xlabel('Average MSE Loss')\n",
        "  plt.ylabel('Epoch')\n",
        "\n",
        "  plt.xlim([min(x), max(x)])\n",
        "\n",
        "  plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7iShoshCpZHI"
      },
      "source": [
        "# Surface Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kybrsKiFu02"
      },
      "source": [
        "## The base class `SurfaceGenerator`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WsepOwreF_k"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "import numpy as np\n",
        "import sympy\n",
        "from scipy import stats\n",
        "\n",
        "class SurfaceGenerator(ABC):\n",
        "    def __init__(self, n_points, rms, skewness, kurtosis, corlength_x, corlength_y, alpha):\n",
        "        self.n_points = n_points\n",
        "        self.rms = rms\n",
        "        self.skewness = skewness\n",
        "        self.kurtosis = kurtosis\n",
        "        self.corlength_x = corlength_x\n",
        "        self.corlength_y = corlength_y\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self._mean = 0\n",
        "        self._length = 0\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.__class__.__name__}({self.n_points}, {self.rms}, {self.skewness}, {self.kurtosis}, {self.corlength_x}, {self.corlength_y}, {self.alpha})\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'<{self}>'\n",
        "\n",
        "    def __call__(self, length):\n",
        "        self._length = length\n",
        "\n",
        "        return self\n",
        "\n",
        "    def __len__(self):\n",
        "      return self._length\n",
        "\n",
        "    def __iter__(self):\n",
        "        for _ in range(self._length):\n",
        "            yield self.generate_surface()\n",
        "\n",
        "    def sort(self, elements):\n",
        "        indices = np.argsort(elements, axis=0)\n",
        "\n",
        "        return elements[indices], indices\n",
        "\n",
        "    @abstractmethod\n",
        "    def autocorrelation(self, tx, ty):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def generate_surface(self):\n",
        "        # 1st step: Generation of a Gaussian surface\n",
        "\n",
        "        # Determine the autocorrelation function R(tx,ty)\n",
        "        R = np.zeros((self.n_points, self.n_points))\n",
        "\n",
        "        txmin = -self.n_points // 2\n",
        "        txmax = self.n_points // 2\n",
        "\n",
        "        tymin = -self.n_points // 2\n",
        "        tymax = self.n_points // 2\n",
        "\n",
        "        dtx = (txmax - txmin) // self.n_points\n",
        "        dty = (tymax-tymin) // self.n_points\n",
        "\n",
        "        for tx in range(txmin, txmax, dtx):\n",
        "            for ty in range(tymin, tymax, dty):\n",
        "                R[tx + txmax, ty + tymax] = self.autocorrelation(tx, ty)\n",
        "\n",
        "        # According to the Wiener-Khinchine theorem FR is the power spectrum of the desired profile\n",
        "        FR = np.fft.fft2(R, (self.n_points, self.n_points))\n",
        "        AMPR = np.sqrt(dtx ** 2 + dty ** 2) * abs(FR)\n",
        "\n",
        "        # 2nd step: Generate a white noise, normalize it and take its Fourier transform\n",
        "        X = np.random.rand(self.n_points, self.n_points)\n",
        "        aveX = np.mean(np.mean((X)))\n",
        "\n",
        "        dif2X = (X - aveX) ** 2\n",
        "        stdX = np.sqrt(np.mean(np.mean(dif2X)))\n",
        "        X = X / stdX\n",
        "        XF = np.fft.fft2(X, s=(self.n_points, self.n_points))\n",
        "\n",
        "        # 3nd step: Multiply the two Fourier transforms\n",
        "        YF = XF * np.sqrt(AMPR)\n",
        "\n",
        "        # 4th step: Perform the inverse Fourier transform of YF and get the desired surface\n",
        "        zaf = np.fft.ifft2(YF, s=(self.n_points, self.n_points))\n",
        "        z = np.real(zaf)\n",
        "\n",
        "        avez = np.mean(np.mean(z))\n",
        "        dif2z = (z-avez) ** 2\n",
        "        stdz = np.sqrt(np.mean(np.mean(dif2z)))\n",
        "        z = ((z - avez) * self.rms) / stdz\n",
        "\n",
        "        # Define the fraction of the surface to be analysed\n",
        "        xmin = 0\n",
        "        xmax = self.n_points\n",
        "        ymin = 0\n",
        "        ymax = self.n_points\n",
        "        z_gs = z[xmin:xmax, ymin:ymax]\n",
        "\n",
        "        # 2nd step: Generation of a non-Gaussian noise NxN\n",
        "        z_ngn = stats.pearson3.rvs(\n",
        "            self.skewness,\n",
        "            loc=self._mean, scale=self.rms, size=(self.n_points, self.n_points)\n",
        "        )\n",
        "\n",
        "        # 3rd step: Combination of z_gs with z_ngn to output a z_ms\n",
        "        v_gs = z_gs.flatten()\n",
        "        v_ngn = z_ngn.flatten()\n",
        "\n",
        "        _, Igs = self.sort(v_gs)\n",
        "\n",
        "        vs_ngn, _ = self.sort(v_ngn)\n",
        "\n",
        "        v_ngs = vs_ngn[Igs]\n",
        "\n",
        "        return v_ngs.reshape(self.n_points, self.n_points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCLThvWIFyMD"
      },
      "source": [
        "## A simple non-gaussian surface generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VAFk69SrV_T"
      },
      "source": [
        "class NonGaussianSurfaceGenerator(SurfaceGenerator):\n",
        "    def __init__(self, n_points=500, rms=1, skewness=0, kurtosis=3, corlength_x=20, corlength_y=20, alpha=1):\n",
        "        super().__init__(n_points, rms, skewness, kurtosis, corlength_x, corlength_y, alpha)\n",
        "\n",
        "    def autocorrelation(self, tx, ty):\n",
        "        return ((self.rms ** 2) * np.exp(-(abs(np.sqrt((tx / self.corlength_x) ** 2 + (ty / self.corlength_y) ** 2))) ** (2 * self.alpha)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie4YBCL_GGx1"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4W0tAWyGIaU"
      },
      "source": [
        "g = NonGaussianSurfaceGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDF7IQJHGU7e"
      },
      "source": [
        "# for surface in g(1):\n",
        "#   as_grayscale_image(surface)\n",
        "#   as_3d_surface(surface)\n",
        "#   plot_correlation(surface)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7jaPDHxhF35G"
      },
      "source": [
        "## A Besel function based non-gaussian surface generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N88uynkFrUSt"
      },
      "source": [
        "class BeselNonGaussianSurfaceGenerator(NonGaussianSurfaceGenerator):\n",
        "    def __init__(self, n_points=500, rms=1, skewness=0, kurtosis=3, corlength_x=20, corlength_y=20, alpha=1, beta_x=1, beta_y=1):\n",
        "        super().__init__(n_points, rms, skewness, kurtosis, corlength_x, corlength_y, alpha)\n",
        "\n",
        "        self.beta_x, self.beta_y = beta_x, beta_y\n",
        "\n",
        "    def autocorrelation(self, tx, ty):\n",
        "        return super().autocorrelation(tx, ty) * sympy.besselj(0, (2 * np.pi * np.sqrt((tx / self.beta_x) ** 2 + (ty / self.beta_y) **2)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMY7zGAfGBtP"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rinj5P0NOmc7"
      },
      "source": [
        "g = BeselNonGaussianSurfaceGenerator()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CH-5d9LlGW0-"
      },
      "source": [
        "# for surface in g(1):\n",
        "#   as_grayscale_image(surface)\n",
        "#   as_3d_surface(surface)\n",
        "#   plot_correlation(surface)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ef2BlNi2PZWo"
      },
      "source": [
        "# Dataset Loading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Moj7a8ezOyv"
      },
      "source": [
        "## Defining the preprocessing pipeline"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_edK95PzTKG"
      },
      "source": [
        "from abc import ABC, abstractmethod\n",
        "\n",
        "class Transform(ABC):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    pass\n",
        "  \n",
        "  @abstractmethod\n",
        "  def __call__(self, *args, **kwargs):\n",
        "    raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyFh-7qNzkLS"
      },
      "source": [
        "from torch import flatten\n",
        "\n",
        "class Flatten(Transform):\n",
        "  def __call__(self, tensor):\n",
        "    return flatten(tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd9ymINl0SkT"
      },
      "source": [
        "class To(Transform):\n",
        "  def __init__(self, device):\n",
        "    self.device = device\n",
        "\n",
        "  def __call__(self, tensor):\n",
        "    return tensor.to(self.device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGhFV9zhzLmN"
      },
      "source": [
        "## Defining the `Dataset` loading procedure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eG3qL7l0Gcer"
      },
      "source": [
        "## The base `NanoroughSurfaceDataset` class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXcHJTxNproO"
      },
      "source": [
        "from torch.utils.data.dataset import  Dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class NanoroughSurfaceDataset(Dataset):\n",
        "  \"\"\"A dataset of pre-generated nanorough surfaces\"\"\"\n",
        "  def __init__(self, surfaces, subsampling_factor=4, transforms=[]):\n",
        "    self.surfaces = np.array(surfaces)\n",
        "    self.surfaces = torch.from_numpy(self.surfaces)\n",
        "\n",
        "    self.subsampling_factor = subsampling_factor\n",
        "    self.subsampling_value = int(surfaces[0].shape[1] / subsampling_factor)\n",
        "\n",
        "    self.transforms = transforms\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.surfaces)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    surface = self.surfaces[idx]\n",
        "\n",
        "    for transform in self.transforms:\n",
        "      surface = transform(surface)\n",
        "\n",
        "    return surface"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R5lPt1z6Gh3R"
      },
      "source": [
        "## A dataset of pre-generated nanorough surfaces in `.mat` format"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3FXvAeySmMU"
      },
      "source": [
        "import scipy.io as sio\n",
        "\n",
        "class NanoroughSurfaceMatLabDataset(NanoroughSurfaceDataset):\n",
        "  \"\"\"A dataset of pre-generated nanorough surfaces in `.mat` format\"\"\"\n",
        "  def __init__(self, surface_dir, subsampling_factor=4, variable_name='data', transforms=[]):\n",
        "    assert surface_dir.is_dir(), \"%s does not exist or is not a dictionary\" % (surface_dir,)\n",
        "\n",
        "    surfaces = []\n",
        "    for file in surface_dir.iterdir():\n",
        "      if file.is_dir() or file.suffix != '.mat':\n",
        "        continue\n",
        "\n",
        "      surfaces.append(self.from_matlab(file, variable_name))\n",
        "\n",
        "    super().__init__(surfaces, subsampling_factor=subsampling_factor, transforms=transforms)\n",
        "\n",
        "  @classmethod\n",
        "  def from_matlab(cls, path_to_mat, variable_name):\n",
        "    matlab_array = sio.loadmat(str(path_to_mat))\n",
        "    numpy_array = matlab_array[variable_name]\n",
        "    \n",
        "    return numpy_array"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og5CT1AHeZeD"
      },
      "source": [
        "# Training and Testing Utilities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SQBkT2ltwo-"
      },
      "source": [
        "## Performing a single training epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKYZzDBfeloB"
      },
      "source": [
        "def train_epoch(generator, discriminator, dataloader, optimizer_generator, optimizer_discriminator, criterion, log_every_n=None, device=None, debug=False):\n",
        "  generator.train()\n",
        "\n",
        "  generator_loss, discriminator_loss, discriminator_output_real, discriminator_output_fake = 0, 0, 0, 0\n",
        "  for train_iteration, X_batch in enumerate(dataloader):\n",
        "    if log_every_n is not None and not train_iteration % log_every_n:\n",
        "      print(f\"Training Iteration #{train_iteration:04d}\")\n",
        "\n",
        "    # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
        "    ## Train with all-real batch\n",
        "    discriminator.zero_grad()\n",
        "    # Format batch\n",
        "    label = torch.full((X_batch.size(0),), 1, dtype=torch.float64, device=device)\n",
        "    # Forward pass real batch through D\n",
        "    output = discriminator(X_batch, debug=debug).view(-1)\n",
        "    # Calculate loss on all-real batch\n",
        "    discriminator_error_real = criterion(output, label)\n",
        "    # Calculate gradients for D in backward pass\n",
        "    discriminator_error_real.backward()\n",
        "    discriminator_output_real_batch = output.mean().item()\n",
        "\n",
        "    ## Train with all-fake batch\n",
        "    # Generate batch of latent vectors\n",
        "    noise = torch.randn(X_batch.size(0), *generator.feature_dims, device=device)\n",
        "    # Generate fake image batch with G\n",
        "    fake = generator(noise, debug=debug)\n",
        "    label.fill_(0)\n",
        "    # Classify all fake batch with D\n",
        "    output = discriminator(fake.detach(), debug=debug).view(-1)\n",
        "    # Calculate D's loss on the all-fake batch\n",
        "    discriminator_error_fake = criterion(output, label)\n",
        "    # Calculate the gradients for this batch\n",
        "    discriminator_error_fake.backward()\n",
        "    # Add the gradients from the all-real and all-fake batches\n",
        "    discriminator_error_total = discriminator_error_real + discriminator_error_fake\n",
        "    # Update D\n",
        "    optimizer_discriminator.step()\n",
        "\n",
        "    # (2) Update G network: maximize log(D(G(z)))\n",
        "    generator.zero_grad()\n",
        "    label.fill_(1)  # fake labels are real for generator cost\n",
        "    # Since we just updated D, perform another forward pass of all-fake batch through D\n",
        "    output = discriminator(fake, debug=debug).view(-1)\n",
        "    # Calculate G's loss based on this output\n",
        "    discriminator_error_fake = criterion(output, label)\n",
        "    # Calculate gradients for G, which propagate through the discriminator\n",
        "    discriminator_error_fake.backward()\n",
        "    discriminator_output_fake_batch = output.mean().item()\n",
        "    # Update G\n",
        "    optimizer_generator.step()\n",
        "\n",
        "    generator_loss += discriminator_error_fake / len(dataloader)\n",
        "    discriminator_loss += discriminator_error_total / len(dataloader)\n",
        "    discriminator_output_real += discriminator_output_real_batch / len(dataloader)\n",
        "    discriminator_output_fake += discriminator_output_fake_batch / len(dataloader)\n",
        "    \n",
        "  return generator_loss, discriminator_loss, discriminator_output_real, discriminator_output_fake"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xUT3qdeOvjAe"
      },
      "source": [
        "## Spliting the original dataset into Training and Testing subsets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtNz0lSlq776"
      },
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "def train_test_split(dataset, train_ratio=0.8):\n",
        "  train_size = int(len(dataset) * train_ratio)\n",
        "  test_size = len(dataset) - train_size\n",
        "\n",
        "  train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "  return train_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIn9Zfp0wSAh"
      },
      "source": [
        "## Configuring the Training and Testing `DataLoader`s"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLetld-1raPp"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def train_test_dataloaders(dataset, train_ratio=0.8, **kwargs):\n",
        "  train_dataset, test_dataset = train_test_split(dataset, train_ratio=train_ratio)\n",
        "\n",
        "  train_dataloader = DataLoader(train_dataset, **kwargs)\n",
        "  test_dataloader = DataLoader(test_dataset, **kwargs)\n",
        "\n",
        "  return train_dataloader, test_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsSyIpQDwW-D"
      },
      "source": [
        "## The Training Manager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kDymUDGftZoO"
      },
      "source": [
        "class Configuration:\n",
        "  def __init__(self, **kwargs):\n",
        "    for key, value in kwargs.items():\n",
        "      if isinstance(value, dict):\n",
        "        value = Configuration(**value)\n",
        "\n",
        "      setattr(self, key, value)\n",
        "  \n",
        "  def to_dict(self):\n",
        "    rv = {}\n",
        "    for key, value in self.__dict__.items():\n",
        "      if isinstance(value, Configuration):\n",
        "        value = value.to_dict()\n",
        "      \n",
        "      rv[key] = value\n",
        "    \n",
        "    return rv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhULZ2lIt90R"
      },
      "source": [
        "from torch.optim import Adam\n",
        "from functools import partial\n",
        "\n",
        "class TrainingManager(Configuration):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    if not hasattr(self, \"debug\"):\n",
        "      self.debug = False\n",
        "\n",
        "    if self.debug is True:\n",
        "      self.verbose = True\n",
        "\n",
        "    if not hasattr(self, \"verbose\"):\n",
        "      self.verbose = False\n",
        "    \n",
        "    if self.verbose is True:\n",
        "      self.benchmark = True\n",
        "\n",
        "    if not hasattr(self, \"benchmark\"):\n",
        "      self.benchmark = False\n",
        "\n",
        "    if not hasattr(self, \"log_every_n\") or self.verbose is False:\n",
        "      self.log_every_n = None\n",
        "    \n",
        "    if not hasattr(self, \"checkpoint_dir\"):\n",
        "      self.checkpoint_dir = None\n",
        "  \n",
        "  def __call__(self, generator, discriminator, dataset):\n",
        "    train_dataloader, test_dataloader = train_test_dataloaders(dataset, train_ratio=self.train_ratio, **self.dataloader.to_dict())\n",
        "\n",
        "    optimizer_generator = Adam(generator.parameters(), **self.optimizer.to_dict())\n",
        "    optimizer_discriminator = Adam(discriminator.parameters(), **self.optimizer.to_dict())\n",
        "\n",
        "    train_epoch_f = self.train_epoch\n",
        "\n",
        "    if self.benchmark is True:\n",
        "      train_epoch_f = benchmark(train_epoch_f)\n",
        "\n",
        "    if self.debug is True:\n",
        "      train_epoch_f = debug(partial(train_epoch_f, debug=True))\n",
        "\n",
        "    generator_losses, discriminator_losses, discriminator_output_reals, discriminator_output_fakes = [], [], [], []\n",
        "    for epoch in range(self.n_epochs):\n",
        "      generator_loss, discriminator_loss, discriminator_output_real, discriminator_output_fake = train_epoch_f(generator, discriminator, train_dataloader, optimizer_generator, optimizer_discriminator, self.criterion, self.log_every_n)\n",
        "      \n",
        "      if self.checkpoint_dir is not None and (not generator_losses or train_loss < min(generator_losses)):\n",
        "        torch.save(generator.state_dict(), self.checkpoint_dir / f'{epoch:03d}.mt')\n",
        "\n",
        "      generator_losses.append(generator_loss)\n",
        "      discriminator_losses.append(discriminator_loss)\n",
        "      discriminator_output_reals.append(discriminator_output_real)\n",
        "      discriminator_output_fakes.append(discriminator_output_fake)\n",
        "      \n",
        "      if self.verbose is True:\n",
        "        print(\"Epoch: %02d, Generator Loss: %7.3f, Discriminator Loss: %7.3f\" % (epoch, generator_loss, discriminator_loss))\n",
        "        print(\"Epoch: %02d, Discriminator Output: [Real: %7.3f, Fake: %7.3f]\" % (epoch, discriminator_output_real, discriminator_output_fake))\n",
        "    \n",
        "    return generator_losses, discriminator_losses, discriminator_output_reals, discriminator_output_fakes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ovsp2b7b7ho"
      },
      "source": [
        "# A naive-approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsKgzXIic12b"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class PerceptronGenerator(nn.Module):\n",
        "  def __init__(self, in_features, out_features, dtype=torch.float64):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_features, self.out_features = in_features, out_features\n",
        "\n",
        "    self.feature_dims = (in_features,)\n",
        "\n",
        "    self.linear = nn.Linear(in_features, out_features).type(dtype)\n",
        "    self.activation = nn.ReLU()\n",
        "  \n",
        "  def forward(self, batch, debug=False):\n",
        "    if debug is True:\n",
        "      print(batch)\n",
        "      \n",
        "    return self.activation(self.linear(batch))\n",
        "  \n",
        "  @classmethod\n",
        "  def from_dataset(cls, dataset, dtype=torch.float64, device=None):\n",
        "    in_features = dataset.subsampling_value ** 2\n",
        "    out_features = (dataset.subsampling_factor * dataset.subsampling_value) ** 2\n",
        "\n",
        "    model =  cls(in_features, out_features, dtype=dtype)\n",
        "\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "      model = nn.DataParallel(model)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7fIWDotoqQe"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class PerceptronDiscriminator(nn.Module):\n",
        "  def __init__(self, in_features, dtype=torch.float64):\n",
        "    super().__init__()\n",
        "\n",
        "    self.feature_dims = (in_features,)\n",
        "\n",
        "    self.linear = nn.Linear(in_features, 1).type(dtype)\n",
        "    self.activation = nn.Sigmoid()\n",
        "  \n",
        "  def forward(self, batch, debug=False):\n",
        "    if debug is True:\n",
        "      print(batch)\n",
        "\n",
        "    return self.activation(self.linear(batch))\n",
        "  \n",
        "  @classmethod\n",
        "  def from_generator(cls, generator, dtype=torch.float64, device=None):\n",
        "    model =  cls(generator.out_features, dtype=dtype)\n",
        "\n",
        "    if torch.cuda.is_available() and torch.cuda.device_count() > 1:\n",
        "      model = nn.DataParallel(model)\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW7CXoinvdeH"
      },
      "source": [
        "## Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pn75Hlnw0Tm"
      },
      "source": [
        "if GDRIVE_DIR.is_dir():\n",
        "  DATASET_PATH = GDRIVE_DIR / 'MyDrive' / 'Thesis' / 'Datasets' / 'surfaces.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-PKd1GWxYQj"
      },
      "source": [
        "SURFACES_DIR = BASE_DIR / 'surfaces'\n",
        "SURFACES_DIR.mkdir(parents=True, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MslJv5kivhIY"
      },
      "source": [
        "from zipfile import ZipFile\n",
        "\n",
        "if DATASET_PATH.is_file():\n",
        "  with ZipFile(DATASET_PATH, 'r') as zip:\n",
        "    zip.extractall(SURFACES_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JuXnxU4e-NA"
      },
      "source": [
        "if SURFACES_DIR.is_dir():\n",
        "  dataset = NanoroughSurfaceMatLabDataset(SURFACES_DIR, transforms=[Flatten(), To(device)])\n",
        "else:\n",
        "  # Here we might automatic surface generation\n",
        "  # raise a `NotImplementedError` for now\n",
        "  raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8K2Yt4Yygnp"
      },
      "source": [
        "## Instantiating the **Generator** and the **Discriminator** Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfhGL9wz_igq"
      },
      "source": [
        "generator = PerceptronGenerator.from_dataset(dataset, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SllxLU-zqqfO"
      },
      "source": [
        "discriminator = PerceptronDiscriminator.from_generator(generator, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Dev3j37tn3x"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhk6EKKvYcSo"
      },
      "source": [
        "from torch.nn import BCELoss\n",
        "\n",
        "criterion = BCELoss().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ky0tb0YFJQoG"
      },
      "source": [
        "from functools import partial\n",
        "\n",
        "training_manager = TrainingManager(\n",
        "    benchmark=True,\n",
        "    verbose=True,\n",
        "    debug=True,\n",
        "    checkpoint_dir=None,\n",
        "    train_epoch=partial(train_epoch, device=device),\n",
        "    log_every_n=10,\n",
        "    criterion=criterion,\n",
        "    n_epochs=10,\n",
        "    train_ratio=0.8,\n",
        "    optimizer={\n",
        "      'lr': 0.001,\n",
        "      'weight_decay': 0\n",
        "    },\n",
        "    dataloader={\n",
        "      'batch_size': 128,\n",
        "      'shuffle': True,\n",
        "      'num_workers': 0,\n",
        "    }\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        },
        "id": "l8DTv6M3JmJ7",
        "outputId": "ef9b37f7-01d6-4f63-fed3-541494007058"
      },
      "source": [
        "model, train_losses, test_losses = training_manager(generator, discriminator, dataset)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Iteration #0000\n",
            "tensor([[ 0.9899,  0.8543,  0.4182,  ...,  1.0619,  1.5418,  1.0455],\n",
            "        [ 0.3082,  0.1400,  0.0048,  ...,  0.3592,  1.1984,  1.2893],\n",
            "        [-0.6707, -0.4744, -0.2135,  ...,  1.8230,  0.3966, -0.1570],\n",
            "        ...,\n",
            "        [-0.2438, -0.2079, -0.4279,  ..., -0.4433, -0.3043,  0.0804],\n",
            "        [-0.8506, -0.5990, -0.0849,  ..., -1.3179, -0.4964, -0.0228],\n",
            "        [-0.7159,  0.0222,  0.0817,  ...,  0.7098,  0.0684, -1.0482]],\n",
            "       device='cuda:0', dtype=torch.float64)\n",
            "tensor([[ 0.3482, -0.9704, -1.4304,  ..., -1.0486,  0.4644, -0.8822],\n",
            "        [-0.8711,  0.3937, -0.2729,  ...,  0.6861,  0.3185, -0.8307],\n",
            "        [-0.4645, -2.4283,  0.2489,  ..., -0.9476, -0.5412, -0.2385],\n",
            "        ...,\n",
            "        [-1.1282, -1.4746,  1.1907,  ...,  0.0145,  1.0656,  0.3169],\n",
            "        [ 0.3159,  0.3231,  0.4856,  ...,  0.3573,  1.8360, -0.4365],\n",
            "        [-0.7868, -0.2171, -0.6506,  ..., -0.1228,  1.4908,  1.7980]],\n",
            "       device='cuda:0')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-222-88f074854cfe>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m           \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-223-7e4855ba025d>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbeg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-242-a4548f41cbc3>\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(generator, discriminator, dataloader, optimizer_generator, optimizer_discriminator, criterion, log_every_n, device, debug)\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Generate fake image batch with G\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mfake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdebug\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n",
            "\u001b[0;32m<ipython-input-247-5ad6cc76c0d6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, batch, debug)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1753\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1754\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Double but found Float",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-257-081427715bc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-246-c21f2057cf2a>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, generator, discriminator, dataset)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mgenerator_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_output_reals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_output_fakes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m       \u001b[0mgenerator_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_output_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_output_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_discriminator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every_n\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheckpoint_dir\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mgenerator_losses\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrain_loss\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_losses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-222-88f074854cfe>\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     22\u001b[0m           \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m           \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{method.__name__}({called_with}) raised {e}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m           \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'functools.partial' object has no attribute '__name__'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW4PP5VLeE7t"
      },
      "source": [
        "train_vs_test(train_losses, test_losses)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}